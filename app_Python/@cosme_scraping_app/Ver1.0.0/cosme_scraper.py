import requests
from bs4 import BeautifulSoup
import pandas as pd
from urllib.parse import urljoin
from datetime import datetime
import re
import os

def scrape_ranking_pages(start_url):
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/122.0.0.0 Safari/537.36"
        )
    }

    all_data = []
    visited_urls = set()
    next_urls = [start_url]
    page_title = "ãƒ©ãƒ³ã‚­ãƒ³ã‚°"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¿ã‚¤ãƒˆãƒ«

    while next_urls:
        url = next_urls.pop(0)
        if url in visited_urls:
            continue
        visited_urls.add(url)

        print(f"ğŸ” {url} ã‚’å‡¦ç†ä¸­...")

        res = requests.get(url, headers=headers)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")

        # âœ… åˆå›ã®ã¿ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
        if page_title == "ãƒ©ãƒ³ã‚­ãƒ³ã‚°":
            title_elem = soup.select_one("#keyword-sp-ttl > div:nth-of-type(2) > h2")
            if title_elem:
                page_title = title_elem.get_text(strip=True)
                # ãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ãˆãªã„æ–‡å­—ã‚’é™¤å»ï¼ˆä¾‹ï¼š/ \ : * ? " < > |ï¼‰
                page_title = re.sub(r'[\\/:*?"<>|]', '', page_title)

        # âœ… ãƒ©ãƒ³ã‚­ãƒ³ã‚°éƒ¨åˆ†ã®æŠ½å‡º
        ranking_section = soup.find("div", id="keyword-ranking-list")
        if not ranking_section:
            continue

        # âœ… å•†å“åã¨URLã‚’æŠ½å‡º
        items = ranking_section.select("h4 a")

        for a in items:
            name = a.get_text(strip=True)
            href = a.get("href")
            if not name or not href:
                continue
            href = urljoin("https://www.cosme.net", href)
            all_data.append([name, href, url])

        # âœ… ãƒ•ãƒƒã‚¿ãƒ¼å†…ã®ãƒªãƒ³ã‚¯ã‚’å·¡å›
        footer = soup.find("div", id="keyword-ranking-footer")
        if footer:
            footer_links = footer.select("a")
            for link in footer_links:
                next_href = link.get("href")
                if next_href:
                    next_url = urljoin(url, next_href)
                    if next_url not in visited_urls:
                        next_urls.append(next_url)

    # âœ… DataFrameä½œæˆ
    df = pd.DataFrame(all_data, columns=["ã‚³ã‚¹ãƒ¡å", "URL", "å–å¾—å…ƒãƒšãƒ¼ã‚¸"])
    df.insert(0, "No", range(1, len(df) + 1))

    return df, page_title


def save_to_excel(df, folder_path, title):
    # âœ… ã‚¿ã‚¤ãƒˆãƒ«ã¨æ—¥æ™‚ã‚’å«ã‚ãŸãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
    now = datetime.now().strftime("%Y%m%d_%H%M")
    file_name = f"@ã‚³ã‚¹ãƒ¡_{title}_{now}.xlsx"

    # âœ… ä¿å­˜å…ˆãƒ‘ã‚¹ã‚’çµåˆ
    file_path = os.path.join(folder_path, file_name)

    df.to_excel(file_path, index=False)
    return file_path
